{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8efaf7922647753",
   "metadata": {},
   "source": [
    "### Scrape from DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 100 papers so far...\n",
      "Retrieved 200 papers so far...\n",
      "Retrieved 300 papers so far...\n",
      "Retrieved 400 papers so far...\n",
      "Retrieved 500 papers so far...\n",
      "Retrieved 600 papers so far...\n",
      "Retrieved 700 papers so far...\n",
      "Retrieved 800 papers so far...\n",
      "Retrieved 900 papers so far...\n",
      "Retrieved 1000 papers so far...\n",
      "Total papers retrieved and saved: 1000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "\n",
    "# Define base URL for DBLP API\n",
    "BASE_URL = 'https://dblp.org/search/publ/api'\n",
    "\n",
    "# Query parameters\n",
    "query_params = {\n",
    "    'q': 'computer science',   # Query for computer science papers\n",
    "    'format': 'xml',           # Response format\n",
    "    'h': 100,                  # Number of results per page (max is 100)\n",
    "    'f': 0                     # Starting index for pagination\n",
    "}\n",
    "\n",
    "# Storage for results\n",
    "all_papers = []\n",
    "total_records = 1000  # Target number of papers\n",
    "records_per_request = 100  # Number of records per request\n",
    "\n",
    "while len(all_papers) < total_records:\n",
    "    # Make API request\n",
    "    response = requests.get(BASE_URL, params=query_params)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse XML response\n",
    "    root = ET.fromstring(response.text)\n",
    "\n",
    "    # Find and store metadata for each paper\n",
    "    for hit in root.findall(\".//hit\"):\n",
    "        paper_data = {\n",
    "            \"title\": hit.find(\".//title\").text,\n",
    "            \"year\": hit.find(\".//year\").text if hit.find(\".//year\") is not None else \"Unknown\",\n",
    "            \"authors\": [author.text for author in hit.findall(\".//author\")],\n",
    "            \"venue\": hit.find(\".//venue\").text if hit.find(\".//venue\") is not None else \"Unknown\",\n",
    "            \"url\": hit.find(\".//ee\").text if hit.find(\".//ee\") is not None else \"No URL\"\n",
    "        }\n",
    "        all_papers.append(paper_data)\n",
    "\n",
    "    print(f\"Retrieved {len(all_papers)} papers so far...\")\n",
    "\n",
    "    # Update for pagination\n",
    "    query_params['f'] += records_per_request  # Increment starting index for next batch\n",
    "\n",
    "    # Respect rate limits\n",
    "    time.sleep(60)  # Add delay to avoid overwhelming the server\n",
    "\n",
    "# Trim to the number of records\n",
    "all_papers = all_papers[:total_records]\n",
    "\n",
    "# Save results to a JSON file\n",
    "import json\n",
    "with open('dblp_computer_science_papers.json', 'w') as f:\n",
    "    json.dump(all_papers, f, indent=2)\n",
    "\n",
    "print(f\"Total papers retrieved and saved: {len(all_papers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e44675c-f5b9-489e-8cd3-8f55115adb2c",
   "metadata": {},
   "source": [
    "### Scrape from Scopus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68fbb82-58b5-4823-901e-3031b90d44a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 25 papers so far...\n",
      "Retrieved 50 papers so far...\n",
      "Retrieved 75 papers so far...\n",
      "Retrieved 100 papers so far...\n",
      "Retrieved 125 papers so far...\n",
      "Retrieved 150 papers so far...\n",
      "Retrieved 175 papers so far...\n",
      "Retrieved 200 papers so far...\n",
      "Retrieved 225 papers so far...\n",
      "Retrieved 250 papers so far...\n",
      "Retrieved 275 papers so far...\n",
      "Retrieved 300 papers so far...\n",
      "Retrieved 325 papers so far...\n",
      "Retrieved 350 papers so far...\n",
      "Retrieved 375 papers so far...\n",
      "Retrieved 400 papers so far...\n",
      "Retrieved 425 papers so far...\n",
      "Retrieved 450 papers so far...\n",
      "Retrieved 475 papers so far...\n",
      "Retrieved 500 papers so far...\n",
      "Retrieved 525 papers so far...\n",
      "Retrieved 550 papers so far...\n",
      "Retrieved 575 papers so far...\n",
      "Retrieved 600 papers so far...\n",
      "Retrieved 625 papers so far...\n",
      "Retrieved 650 papers so far...\n",
      "Retrieved 675 papers so far...\n",
      "Retrieved 700 papers so far...\n",
      "Retrieved 725 papers so far...\n",
      "Retrieved 750 papers so far...\n",
      "Retrieved 775 papers so far...\n",
      "Retrieved 800 papers so far...\n",
      "Retrieved 825 papers so far...\n",
      "Retrieved 850 papers so far...\n",
      "Retrieved 875 papers so far...\n",
      "Retrieved 900 papers so far...\n",
      "Retrieved 925 papers so far...\n",
      "Retrieved 950 papers so far...\n",
      "Retrieved 975 papers so far...\n",
      "Retrieved 1000 papers so far...\n",
      "Total papers retrieved: 1000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set up your API key and base URL\n",
    "API_KEY = os.getenv('ELSEVIER_API_KEY')\n",
    "BASE_URL = 'https://api.elsevier.com/content/search/scopus'\n",
    "\n",
    "# Define query parameters for computer science papers\n",
    "query_params = {\n",
    "    'query': 'SUBJAREA(COMP)',     # Scopus field code for computer science\n",
    "    'count': 25,                   # Max 25 results per request\n",
    "    'start': 0,                     # Starting index for pagination\n",
    "    'sort': 'citedby-count',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'X-ELS-APIKey': API_KEY\n",
    "}\n",
    "\n",
    "# Initialize storage for results\n",
    "all_papers = []\n",
    "total_records = 1000               # Target number of records\n",
    "records_per_request = 25           # Number of records per request\n",
    "\n",
    "# Retrieve data in batches\n",
    "while len(all_papers) < total_records:\n",
    "    # Make API request\n",
    "    response = requests.get(BASE_URL, headers=headers, params=query_params)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract and store results\n",
    "    if 'search-results' in data:\n",
    "        entries = data['search-results'].get('entry', [])\n",
    "        all_papers.extend(entries)\n",
    "        print(f\"Retrieved {len(all_papers)} papers so far...\")\n",
    "\n",
    "        # Update start index for the next batch\n",
    "        query_params['start'] += records_per_request\n",
    "    else:\n",
    "        print(\"Error retrieving data:\", data)\n",
    "        break\n",
    "\n",
    "# Display summary of results\n",
    "print(f\"Total papers retrieved: {len(all_papers)}\")\n",
    "\n",
    "# Save results to a file\n",
    "import json\n",
    "with open('scopus_computer_science_papers.json', 'w') as f:\n",
    "    json.dump(all_papers, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1c1940-ae2e-41c2-977d-e18b21db821c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
