{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8efaf7922647753",
   "metadata": {},
   "source": [
    "### Scraping from DBLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 100 papers so far...\n",
      "Retrieved 200 papers so far...\n",
      "Retrieved 300 papers so far...\n",
      "Retrieved 400 papers so far...\n",
      "Retrieved 500 papers so far...\n",
      "Retrieved 600 papers so far...\n",
      "Retrieved 700 papers so far...\n",
      "Retrieved 800 papers so far...\n",
      "Retrieved 900 papers so far...\n",
      "Retrieved 1000 papers so far...\n",
      "Total papers retrieved and saved: 1000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "\n",
    "# Define base URL for DBLP API\n",
    "BASE_URL = 'https://dblp.org/search/publ/api'\n",
    "\n",
    "# Query parameters\n",
    "query_params = {\n",
    "    'q': 'computer science',   # Query for computer science papers\n",
    "    'format': 'xml',           # Response format\n",
    "    'h': 100,                  # Number of results per page (max is 100)\n",
    "    'f': 0                     # Starting index for pagination\n",
    "}\n",
    "\n",
    "# Storage for results\n",
    "all_papers = []\n",
    "total_records = 1000  # Target number of papers\n",
    "records_per_request = 100  # Number of records per request\n",
    "\n",
    "while len(all_papers) < total_records:\n",
    "    # Make API request\n",
    "    response = requests.get(BASE_URL, params=query_params)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Parse XML response\n",
    "    root = ET.fromstring(response.text)\n",
    "\n",
    "    # Find and store metadata for each paper\n",
    "    for hit in root.findall(\".//hit\"):\n",
    "        paper_data = {\n",
    "            \"title\": hit.find(\".//title\").text,\n",
    "            \"year\": hit.find(\".//year\").text if hit.find(\".//year\") is not None else \"Unknown\",\n",
    "            \"authors\": [author.text for author in hit.findall(\".//author\")],\n",
    "            \"venue\": hit.find(\".//venue\").text if hit.find(\".//venue\") is not None else \"Unknown\",\n",
    "            \"url\": hit.find(\".//ee\").text if hit.find(\".//ee\") is not None else \"No URL\"\n",
    "        }\n",
    "        all_papers.append(paper_data)\n",
    "\n",
    "    print(f\"Retrieved {len(all_papers)} papers so far...\")\n",
    "\n",
    "    # Update for pagination\n",
    "    query_params['f'] += records_per_request  # Increment starting index for next batch\n",
    "\n",
    "    # Respect rate limits\n",
    "    time.sleep(60)  # Add delay to avoid overwhelming the server\n",
    "\n",
    "# Trim to the number of records\n",
    "all_papers = all_papers[:total_records]\n",
    "\n",
    "# Save results to a JSON file\n",
    "import json\n",
    "with open('dblp_computer_science_papers.json', 'w') as f:\n",
    "    json.dump(all_papers, f, indent=2)\n",
    "\n",
    "print(f\"Total papers retrieved and saved: {len(all_papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68fbb82-58b5-4823-901e-3031b90d44a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
